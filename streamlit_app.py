import streamlit as st
import pandas as pd
import numpy as np
import openai
import faiss
import pickle
import json
import PyPDF2
import io
import requests
import os
from typing import List, Dict, Tuple, Optional
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime
import base64

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="SRO Risk Analyzer",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

@st.cache_data
def download_sro_files():
    """
    Download autom√°tico apenas do arquivo grande (Embeddings_SRO.index)
    O arquivo Dados_SRO.pkl j√° est√° no reposit√≥rio GitHub
    """
    
    # Configura√ß√£o do arquivo grande no Google Drive
    embeddings_info = {
        "id": "1EHrakmYbVCD6E_aEzhmbMp17stHEw9lU",  # ‚úÖ ID do seu arquivo
        "filename": "Embeddings_SRO.index",
        "size_mb": "~150MB"
    }
    
    # Verificar se o arquivo pequeno existe no reposit√≥rio
    if not os.path.exists("Dados_SRO.pkl"):
        st.error("""
        ‚ùå **Arquivo Dados_SRO.pkl n√£o encontrado**
        
        Este arquivo deveria estar no reposit√≥rio GitHub.
        Certifique-se de que voc√™ fez o upload correto.
        """)
        return False
    else:
        st.success("‚úÖ Dados_SRO.pkl encontrado no reposit√≥rio")
    
    # Verificar se precisa baixar o arquivo grande
    if not os.path.exists(embeddings_info["filename"]):
        # Tentar diferentes URLs do Google Drive
        urls_to_try = [
            f"https://drive.google.com/uc?export=download&id={embeddings_info['id']}",
            f"https://drive.google.com/uc?id={embeddings_info['id']}&export=download",
            f"https://drive.usercontent.google.com/download?id={embeddings_info['id']}&export=download&authuser=0&confirm=t"
        ]
        
        with st.spinner(f"üì• Baixando {embeddings_info['filename']} ({embeddings_info['size_mb']})..."):
            download_success = False
            
            for i, file_url in enumerate(urls_to_try):
                try:
                    st.info(f"Tentativa {i+1}/3: Baixando do Google Drive...")
                    
                    # Fazer request inicial
                    session = requests.Session()
                    response = session.get(file_url, stream=True)
                    
                    # Verificar se precisa confirmar download (arquivos grandes)
                    if 'download_warning' in response.text or 'virus scan warning' in response.text:
                        # Extrair token de confirma√ß√£o
                        import re
                        confirm_token = None
                        for line in response.text.splitlines():
                            if 'confirm=' in line:
                                confirm_token = re.search(r'confirm=([^&]*)', line)
                                if confirm_token:
                                    confirm_token = confirm_token.group(1)
                                    break
                        
                        if confirm_token:
                            # Fazer download com confirma√ß√£o
                            params = {'id': embeddings_info['id'], 'confirm': confirm_token}
                            response = session.get('https://drive.google.com/uc', params=params, stream=True)
                    
                    response.raise_for_status()
                    
                    # Verificar se √© um arquivo v√°lido (n√£o p√°gina de erro)
                    content_type = response.headers.get('content-type', '')
                    if 'text/html' in content_type and response.status_code == 200:
                        # Pode ser p√°gina de confirma√ß√£o, tentar pr√≥xima URL
                        continue
                    
                    # Salvar arquivo com progresso
                    total_size = int(response.headers.get('content-length', 0))
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    with open(embeddings_info["filename"], 'wb') as f:
                        downloaded = 0
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded += len(chunk)
                                if total_size > 0:
                                    progress = downloaded / total_size
                                    progress_bar.progress(progress)
                                    status_text.text(f"Baixado: {downloaded / (1024*1024):.1f} MB de {total_size / (1024*1024):.1f} MB")
                    
                    # Verificar se download foi bem-sucedido
                    if os.path.exists(embeddings_info["filename"]) and os.path.getsize(embeddings_info["filename"]) > 10000000:  # > 10MB
                        st.success(f"‚úÖ {embeddings_info['filename']} baixado com sucesso!")
                        download_success = True
                        break
                    else:
                        st.warning(f"‚ö†Ô∏è Tentativa {i+1} falhou, tentando pr√≥xima URL...")
                        if os.path.exists(embeddings_info["filename"]):
                            os.remove(embeddings_info["filename"])
                        
                except requests.RequestException as e:
                    st.warning(f"‚ö†Ô∏è Tentativa {i+1} falhou: {str(e)}")
                    continue
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Tentativa {i+1} erro inesperado: {str(e)}")
                    continue
            
            if not download_success:
                st.error(f"""
                ‚ùå **Falha no Download: {embeddings_info['filename']}**
                
                N√£o foi poss√≠vel baixar o arquivo do Google Drive.
                
                **Solu√ß√µes poss√≠veis:**
                
                1. **Verificar permiss√µes do Google Drive:**
                   - Acesse: https://drive.google.com/file/d/{embeddings_info['id']}/view
                   - Certifique-se que est√° definido como "Anyone with the link can view"
                
                2. **Download manual:**
                   - Baixe o arquivo manualmente do link acima
                   - Fa√ßa upload direto no reposit√≥rio GitHub (usando Git LFS)
                
                3. **Arquivo muito grande:**
                   - Google Drive pode bloquear downloads autom√°ticos de arquivos > 100MB
                   - Considere dividir o arquivo em partes menores
                
                **ID atual:** {embeddings_info['id']}
                """)
                return False
    else:
        st.info(f"üìÅ {embeddings_info['filename']} j√° existe localmente")
    
    return True

def check_files_status():
    """Verifica status dos arquivos SRO"""
    required_files = ["Embeddings_SRO.index", "Dados_SRO.pkl"]
    files_status = {}
    
    for file in required_files:
        exists = os.path.exists(file)
        size = os.path.getsize(file) if exists else 0
        source = "GitHub" if file == "Dados_SRO.pkl" else "Google Drive"
        
        files_status[file] = {
            "exists": exists,
            "size_mb": round(size / (1024*1024), 1) if exists else 0,
            "source": source
        }
    
    return files_status

class SROAnalyzer:
    """Classe principal para an√°lise de risco de reclama√ß√µes SRO"""
    
    def __init__(self):
        self.faiss_index = None
        self.data_list = None
        self.client = None
        self.is_loaded = False
        
    def load_system(self, api_key: str) -> bool:
        """Carrega o sistema FAISS e OpenAI"""
        try:
            # Configurar OpenAI
            self.client = openai.OpenAI(api_key=api_key)
            
            # Carregar √≠ndice FAISS
            self.faiss_index = faiss.read_index("Embeddings_SRO.index")
            
            # Carregar dados correspondentes
            with open("Dados_SRO.pkl", 'rb') as f:
                self.data_list = pickle.load(f)
            
            self.is_loaded = True
            return True
            
        except Exception as e:
            st.error(f"Erro ao carregar sistema: {str(e)}")
            return False
    
    def analyze_sentiment(self, text: str) -> Dict:
        """Analisa sentimento do texto usando OpenAI"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "system", 
                        "content": """Voc√™ √© um especialista em an√°lise de sentimento para atendimento ao cliente.
                        
Analise o texto e classifique o sentimento em uma escala de -1 a +1:
- -1.0: Muito negativo (reclama√ß√£o s√©ria, raiva, insatisfa√ß√£o extrema)
- -0.5: Negativo (insatisfa√ß√£o, problema relatado)
- 0.0: Neutro (informativo, sem emo√ß√£o clara)
- +0.5: Positivo (satisfa√ß√£o, agradecimento)
- +1.0: Muito positivo (elogio, satisfa√ß√£o extrema)

Responda APENAS com um n√∫mero decimal entre -1 e +1, exemplo: 0.7"""
                    },
                    {
                        "role": "user",
                        "content": f"Analise o sentimento deste texto: '{text}'"
                    }
                ],
                temperature=0.1,
                max_tokens=10
            )
            
            # Tentar converter resposta para float
            response_text = response.choices[0].message.content.strip()
            
            # Limpar poss√≠veis caracteres extras
            import re
            numbers = re.findall(r'-?\d+\.?\d*', response_text)
            
            if numbers:
                sentiment_score = float(numbers[0])
            else:
                # Se n√£o conseguir extrair n√∫mero, assumir neutro
                sentiment_score = 0.0
            
            # Garantir que est√° no range correto
            sentiment_score = max(-1.0, min(1.0, sentiment_score))
            
            # Classificar sentimento
            if sentiment_score >= 0.5:
                sentiment_label = "Muito Positivo"
                sentiment_color = "#00C851"
            elif sentiment_score >= 0.1:
                sentiment_label = "Positivo"
                sentiment_color = "#4CAF50"
            elif sentiment_score >= -0.1:
                sentiment_label = "Neutro"
                sentiment_color = "#FFC107"
            elif sentiment_score >= -0.5:
                sentiment_label = "Negativo"
                sentiment_color = "#FF8C00"
            else:
                sentiment_label = "Muito Negativo"
                sentiment_color = "#FF4B4B"
            
            return {
                "score": sentiment_score,
                "label": sentiment_label,
                "color": sentiment_color
            }
            
        except Exception as e:
            st.warning(f"Erro na an√°lise de sentimento: {str(e)}. Usando an√°lise neutro.")
            # Retornar neutro em caso de erro
            return {
                "score": 0.0,
                "label": "Neutro (Erro)",
                "color": "#FFC107"
            }
        """Gera embedding para um texto"""
        try:
            response = self.client.embeddings.create(
                input=text,
                model="text-embedding-ada-002"
            )
            return response.data[0].embedding
        except Exception as e:
            st.error(f"Erro ao gerar embedding: {str(e)}")
            return None
    
    def analyze_risk(self, text: str, top_k: int = 10) -> Dict:
        """Analisa risco de reclama√ß√£o baseado em similaridade E sentimento"""
        if not self.is_loaded:
            return {"error": "Sistema n√£o carregado"}
        
        # 1. AN√ÅLISE DE SENTIMENTO
        sentiment = self.analyze_sentiment(text)
        
        # 2. GERAR EMBEDDING E BUSCAR SIMILARES
        embedding = self.generate_embedding(text)
        if embedding is None:
            return {"error": "Falha ao gerar embedding"}
        
        # Buscar similares
        query_vector = np.array([embedding], dtype=np.float32)
        faiss.normalize_L2(query_vector)
        
        distances, indices = self.faiss_index.search(query_vector, top_k)
        
        # Analisar resultados
        similarities = distances[0]
        max_similarity = float(similarities[0]) if len(similarities) > 0 else 0.0
        avg_similarity = float(np.mean(similarities))
        
        # 3. CALCULAR SCORE DE RISCO AJUSTADO PELO SENTIMENTO
        base_risk = max_similarity * 100  # Score baseado em similaridade
        
        # Ajuste baseado no sentimento
        if sentiment["score"] >= 0.3:  # Texto positivo
            # Reduzir drasticamente o risco para textos positivos
            sentiment_multiplier = max(0.1, 1 - abs(sentiment["score"]))  # 0.1 a 0.7
            adjusted_risk = base_risk * sentiment_multiplier
            risk_explanation = "Risco reduzido devido ao sentimento positivo"
            
        elif sentiment["score"] <= -0.3:  # Texto negativo
            # Manter ou aumentar ligeiramente o risco para textos negativos
            sentiment_multiplier = min(1.2, 1 + abs(sentiment["score"]) * 0.3)  # 1.0 a 1.2
            adjusted_risk = base_risk * sentiment_multiplier
            risk_explanation = "Risco aumentado devido ao sentimento negativo"
            
        else:  # Texto neutro
            adjusted_risk = base_risk * 0.8  # Redu√ß√£o moderada para textos neutros
            risk_explanation = "Risco moderado para texto neutro"
        
        # Garantir que o score final est√° entre 0-100
        final_risk_score = max(0, min(100, adjusted_risk))
        
        # 4. CLASSIFICAR RISCO FINAL
        if final_risk_score >= 80:
            risk_level = "Alta"
            risk_color = "#FF4B4B"
        elif final_risk_score >= 60:
            risk_level = "M√©dia"
            risk_color = "#FF8C00"
        elif final_risk_score >= 30:
            risk_level = "Baixa"
            risk_color = "#FFD700"
        else:
            risk_level = "Nula"
            risk_color = "#00C851"
        
        # Obter reclama√ß√µes similares
        similar_complaints = []
        for i, (sim, idx) in enumerate(zip(similarities, indices[0])):
            if idx < len(self.data_list):
                item = self.data_list[idx].copy()
                item['similaridade'] = float(sim)
                item['rank'] = i + 1
                similar_complaints.append(item)
        
        return {
            "risk_score": final_risk_score,
            "risk_level": risk_level,
            "risk_color": risk_color,
            "sentiment": sentiment,
            "base_risk": base_risk,
            "risk_explanation": risk_explanation,
            "max_similarity": max_similarity,
            "avg_similarity": avg_similarity,
            "similar_complaints": similar_complaints,
            "total_analyzed": len(similarities)
        }

def extract_text_from_file(uploaded_file) -> str:
    """Extrai texto de arquivos uploaded"""
    try:
        file_type = uploaded_file.type
        
        if file_type == "application/pdf":
            # Extrair texto de PDF
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
            
        elif file_type in ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", 
                          "application/vnd.ms-excel"]:
            # Extrair texto de Excel
            df = pd.read_excel(uploaded_file)
            text = ""
            for col in df.columns:
                text += f"{col}: {' '.join(df[col].astype(str).tolist())}\n"
            return text
            
        elif file_type == "application/json":
            # Extrair texto de JSON
            json_data = json.load(uploaded_file)
            text = json.dumps(json_data, ensure_ascii=False, indent=2)
            return text
            
        elif file_type == "text/plain":
            # Arquivo de texto
            return str(uploaded_file.read(), "utf-8")
            
        else:
            return "Tipo de arquivo n√£o suportado"
            
    except Exception as e:
        return f"Erro ao extrair texto: {str(e)}"

def create_risk_gauge(risk_score: float, risk_level: str, risk_color: str):
    """Cria gauge de risco"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number+delta",
        value = risk_score,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "Risco de Reclama√ß√£o"},
        delta = {'reference': 50},
        gauge = {
            'axis': {'range': [None, 100]},
            'bar': {'color': risk_color},
            'steps': [
                {'range': [0, 30], 'color': "#E8F5E8"},
                {'range': [30, 60], 'color': "#FFF8DC"},
                {'range': [60, 80], 'color': "#FFE4B5"},
                {'range': [80, 100], 'color': "#FFE4E1"}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 90
            }
        }
    ))
    
    fig.update_layout(
        height=300,
        font={'color': "darkblue", 'family': "Arial"}
    )
    
    return fig

def create_similarity_chart(similar_complaints: List[Dict]):
    """Cria gr√°fico de similaridade"""
    if not similar_complaints:
        return None
    
    df = pd.DataFrame(similar_complaints[:5])  # Top 5
    
    fig = px.bar(
        df, 
        x='rank', 
        y='similaridade',
        title="Top 5 Reclama√ß√µes Similares",
        labels={'similaridade': 'Similaridade (%)', 'rank': 'Ranking'},
        color='similaridade',
        color_continuous_scale='Reds'
    )
    
    fig.update_layout(height=400)
    return fig

def download_report(analysis_result: Dict, original_text: str) -> str:
    """Gera relat√≥rio para download"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "texto_analisado": original_text[:500] + "..." if len(original_text) > 500 else original_text,
        "analise_risco": {
            "score_risco": analysis_result["risk_score"],
            "nivel_risco": analysis_result["risk_level"],
            "similaridade_maxima": analysis_result["max_similarity"],
            "similaridade_media": analysis_result["avg_similarity"]
        },
        "reclamacoes_similares": analysis_result["similar_complaints"][:5]
    }
    
    return json.dumps(report, ensure_ascii=False, indent=2)

# Interface Streamlit
def main():
    # Header
    st.title("üîç SRO Risk Analyzer")
    st.markdown("**Sistema de An√°lise de Risco de Reclama√ß√µes**")
    st.markdown("---")
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes")
        
        # Verificar API Key nos secrets
        try:
            api_key = st.secrets["OPENAI_API_KEY"]
            st.success("üîë API Key carregada dos secrets")
        except KeyError:
            st.error("üîë API Key n√£o encontrada nos secrets")
            st.info("""
            **Configure nos Secrets do Streamlit:**
            1. V√° em Settings > Secrets
            2. Adicione: OPENAI_API_KEY = "sua_chave_aqui"
            """)
            api_key = None
        
        # Status dos arquivos
        st.header("üìÅ Status dos Arquivos SRO")
        
        # Verificar e baixar arquivos se necess√°rio
        files_downloaded = download_sro_files()
        
        if files_downloaded:
            files_status = check_files_status()
            
            for file, status in files_status.items():
                if status["exists"]:
                    st.success(f"‚úÖ {file} ({status['size_mb']} MB) - {status['source']}")
                else:
                    st.error(f"‚ùå {file} n√£o encontrado")
            
            all_files_exist = all(status["exists"] for status in files_status.values())
        else:
            all_files_exist = False
            st.error("‚ùå Falha no download dos arquivos SRO")
        
        # Status geral do sistema
        st.header("üö¶ Status do Sistema")
        if all_files_exist and api_key:
            st.success("üü¢ Sistema Pronto")
        elif all_files_exist and not api_key:
            st.warning("üü° Configure API Key")
        elif not all_files_exist and api_key:
            st.warning("üü° Arquivos em download")
        else:
            st.error("üî¥ Sistema n√£o configurado")
    
    # Verificar pr√©-requisitos
    if not api_key:
        st.error("üîë API Key n√£o configurada nos secrets do Streamlit")
        st.info("""
        **Como configurar:**
        1. Acesse as configura√ß√µes da app no Streamlit Cloud
        2. V√° em "Settings" > "Secrets"
        3. Adicione:
        ```toml
        OPENAI_API_KEY = "sua_chave_openai_aqui"
        ```
        4. Salve e a app ser√° reiniciada automaticamente
        """)
        st.stop()
    
    if not all_files_exist:
        st.error("üìÅ Arquivos SRO n√£o dispon√≠veis")
        st.info("""
        **O que est√° acontecendo:**
        - Os arquivos de embeddings est√£o sendo baixados automaticamente
        - Este processo pode levar alguns minutos na primeira execu√ß√£o
        - Aguarde o download completar ou verifique a configura√ß√£o dos IDs no c√≥digo
        """)
        
        if st.button("üîÑ Tentar Download Novamente"):
            st.rerun()
        
        st.stop()
    
    # Inicializar analyzer
    @st.cache_resource
    def load_analyzer(_api_key):
        analyzer = SROAnalyzer()
        if analyzer.load_system(_api_key):
            return analyzer
        return None
    
    with st.spinner("ü§ñ Carregando sistema de an√°lise..."):
        analyzer = load_analyzer(api_key)
    
    if analyzer is None:
        st.error("‚ùå Falha ao carregar o sistema SRO")
        st.info("Verifique se os arquivos foram baixados corretamente e se a API Key est√° v√°lida")
        st.stop()
    
    st.success("‚úÖ Sistema SRO carregado com sucesso!")
    st.info(f"üìä Base de dados: {len(analyzer.data_list)} reclama√ß√µes hist√≥ricas")
    
    # Interface principal
    tab1, tab2, tab3 = st.tabs(["üì§ Upload de Arquivo", "‚úçÔ∏è Texto Manual", "‚ÑπÔ∏è Sobre"])
    
    with tab1:
        st.header("üì§ An√°lise de Arquivo")
        st.markdown("Fa√ßa upload de um arquivo para analisar o risco de reclama√ß√£o")
        
        uploaded_file = st.file_uploader(
            "Escolha um arquivo",
            type=['pdf', 'xlsx', 'xls', 'json', 'txt'],
            help="Formatos suportados: PDF, Excel, JSON, TXT"
        )
        
        if uploaded_file:
            with st.spinner("üîÑ Extraindo texto do arquivo..."):
                extracted_text = extract_text_from_file(uploaded_file)
            
            # Mostrar preview do texto
            with st.expander("üëÅÔ∏è Preview do Texto Extra√≠do"):
                st.text_area(
                    "Texto extra√≠do:",
                    extracted_text[:1000] + "..." if len(extracted_text) > 1000 else extracted_text,
                    height=200
                )
            
            if st.button("üîç Analisar Risco", key="analyze_file"):
                analyze_text(analyzer, extracted_text, uploaded_file.name)
    
    with tab2:
        st.header("‚úçÔ∏è An√°lise de Texto Manual")
        st.markdown("Digite ou cole um texto para analisar")
        
        manual_text = st.text_area(
            "Digite o texto para an√°lise:",
            height=200,
            placeholder="Cole aqui o texto que deseja analisar..."
        )
        
        if manual_text and st.button("üîç Analisar Risco", key="analyze_manual"):
            analyze_text(analyzer, manual_text, "Texto Manual")
    
    with tab3:
        st.header("‚ÑπÔ∏è Sobre o Sistema")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üéØ Funcionalidades")
            st.write("""
            - **An√°lise de Risco**: Score de 0-100%
            - **Classifica√ß√£o**: Alta/M√©dia/Baixa/Nula
            - **Busca Sem√¢ntica**: IA para encontrar similares
            - **M√∫ltiplos Formatos**: PDF, Excel, JSON, TXT
            - **Relat√≥rios**: Download em JSON
            """)
            
            st.subheader("üõ†Ô∏è Tecnologia")
            st.write("""
            - **IA**: OpenAI Embeddings
            - **Busca**: FAISS (Facebook AI)
            - **Interface**: Streamlit
            - **Base**: 30K+ reclama√ß√µes hist√≥ricas
            """)
        
        with col2:
            st.subheader("üìä Como Interpretar")
            
            # Tabela de interpreta√ß√£o
            interpretation_data = {
                "Score": ["80-100%", "60-79%", "30-59%", "0-29%"],
                "Classifica√ß√£o": ["üî¥ Alta", "üü† M√©dia", "üü° Baixa", "üü¢ Nula"],
                "A√ß√£o": ["Imediata", "Monitoramento", "Observa√ß√£o", "Normal"]
            }
            
            st.dataframe(
                pd.DataFrame(interpretation_data),
                use_container_width=True,
                hide_index=True
            )
            
            st.subheader("üîó Links √öteis")
            st.write("""
            - [OpenAI API](https://platform.openai.com/)
            - [Documenta√ß√£o FAISS](https://faiss.ai/)
            - [Streamlit Docs](https://docs.streamlit.io/)
            """)


def analyze_text(analyzer: SROAnalyzer, text: str, source_name: str):
    """Fun√ß√£o para analisar texto e mostrar resultados"""
    
    with st.spinner("ü§ñ Analisando risco de reclama√ß√£o..."):
        result = analyzer.analyze_risk(text, top_k=10)
    
    if "error" in result:
        st.error(f"‚ùå {result['error']}")
        return
    
    # Verificar se sentiment existe no resultado
    if "sentiment" not in result:
        st.error("‚ùå Erro na an√°lise de sentimento")
        return
    
    # Layout em colunas
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìä Resultado da An√°lise")
        
        # Mostrar an√°lise de sentimento
        st.write("**üé≠ An√°lise de Sentimento:**")
        sentiment_col1, sentiment_col2 = st.columns([1, 2])
        
        with sentiment_col1:
            # Mostrar score de sentimento
            sentiment_score = result["sentiment"]["score"]
            st.metric(
                "Sentimento", 
                f"{sentiment_score:+.2f}",
                help="Escala: -1 (muito negativo) a +1 (muito positivo)"
            )
        
        with sentiment_col2:
            # Mostrar classifica√ß√£o do sentimento
            st.markdown(f"""
            <div style="padding: 0.5rem; border-radius: 0.5rem; background-color: {result['sentiment']['color']}20; border-left: 4px solid {result['sentiment']['color']};">
                <strong style="color: {result['sentiment']['color']};">{result['sentiment']['label']}</strong>
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown("---")
        
        # Gauge de risco
        gauge_fig = create_risk_gauge(
            result["risk_score"], 
            result["risk_level"], 
            result["risk_color"]
        )
        st.plotly_chart(gauge_fig, use_container_width=True)
        
        # M√©tricas principais
        st.metric("üìà Score de Risco Final", f"{result['risk_score']:.1f}%")
        st.metric("üè∑Ô∏è Classifica√ß√£o", result["risk_level"])
        
        # Explica√ß√£o do ajuste (se dispon√≠vel)
        if "risk_explanation" in result:
            st.info(f"‚ÑπÔ∏è {result['risk_explanation']}")
        
        # M√©tricas t√©cnicas em expander
        with st.expander("üîß Detalhes T√©cnicos"):
            col_tech1, col_tech2 = st.columns(2)
            with col_tech1:
                if "base_risk" in result:
                    st.metric("üìä Risco Base (Similaridade)", f"{result['base_risk']:.1f}%")
                st.metric("üîó Similaridade M√°xima", f"{result['max_similarity']:.3f}")
            with col_tech2:
                st.metric("üìä Similaridade M√©dia", f"{result['avg_similarity']:.3f}")
                st.metric("üîç Reclama√ß√µes Analisadas", result['total_analyzed'])
    
    with col2:
        st.subheader("üìà An√°lise de Similaridade")
        
        # Gr√°fico de similaridade
        if result["similar_complaints"]:
            sim_chart = create_similarity_chart(result["similar_complaints"])
            if sim_chart:
                st.plotly_chart(sim_chart, use_container_width=True)
        
        # Estat√≠sticas
        st.write("**üìä Estat√≠sticas:**")
        st.write(f"‚Ä¢ Reclama√ß√µes analisadas: {result['total_analyzed']}")
        st.write(f"‚Ä¢ Similaridade m√©dia: {result['avg_similarity']:.3f}")
    
    # Detalhes das reclama√ß√µes similares
    st.subheader("üîç Reclama√ß√µes Similares Encontradas")
    
    if result["similar_complaints"]:
        for i, complaint in enumerate(result["similar_complaints"][:5]):
            with st.expander(f"#{i+1} - Similaridade: {complaint['similaridade']:.1%}"):
                st.write("**Reclama√ß√£o:**")
                st.write(complaint['reclamacao'])
                st.write("**Solu√ß√£o:**")
                st.write(complaint['solucao'])
    else:
        st.info("Nenhuma reclama√ß√£o similar encontrada")
    
    # Download do relat√≥rio
    st.subheader("üì• Download do Relat√≥rio")
    
    report_json = download_report(result, text)
    
    st.download_button(
        label="üìÑ Baixar Relat√≥rio JSON",
        data=report_json,
        file_name=f"relatorio_risco_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )
    
    # Recomenda√ß√µes inteligentes baseadas em sentimento
    st.subheader("üí° Recomenda√ß√µes")
    
    # Verificar se campos de sentimento existem
    if "sentiment" in result and "score" in result["sentiment"]:
        sentiment_score = result["sentiment"]["score"]
        risk_score = result["risk_score"]
        
        if sentiment_score >= 0.3:  # Texto positivo
            if risk_score < 30:
                st.success("üåü **FEEDBACK POSITIVO**: Este √© um elogio! Considere usar como case de sucesso ou testim√¥nio.")
            else:
                st.info("ü§î **AN√ÅLISE MISTA**: Sentimento positivo com alta similaridade a reclama√ß√µes. Verifique contexto.")
        
        elif sentiment_score <= -0.3:  # Texto negativo
            if risk_score >= 80:
                st.error("üö® **RISCO CR√çTICO**: Sentimento negativo + alta similaridade. A√ß√£o imediata necess√°ria!")
            elif risk_score >= 60:
                st.warning("‚ö†Ô∏è **RISCO ELEVADO**: Monitoramento cont√≠nuo e a√ß√µes preventivas recomendadas.")
            elif risk_score >= 30:
                st.info("üìã **ATEN√á√ÉO**: Sentimento negativo, mas baixa similaridade. Investigar contexto espec√≠fico.")
            else:
                st.success("‚úÖ **RISCO CONTROLADO**: Apesar do tom, baixa probabilidade de reclama√ß√£o formal.")
        
        else:  # Texto neutro
            if risk_score >= 80:
                st.warning("‚ö†Ô∏è **RISCO MODERADO**: Texto neutro com alta similaridade. Monitoramento recomendado.")
            elif risk_score >= 60:
                st.info("‚ÑπÔ∏è **OBSERVA√á√ÉO**: Monitoramento regular suficiente.")
            else:
                st.success("‚úÖ **SITUA√á√ÉO NORMAL**: Procedimentos padr√£o adequados.")
    else:
        # Fallback para recomenda√ß√µes b√°sicas se sentimento n√£o dispon√≠vel
        if result["risk_score"] >= 80:
            st.error("üö® **RISCO ALTO**: Aten√ß√£o imediata necess√°ria. Implementar medidas preventivas urgentes.")
        elif result["risk_score"] >= 60:
            st.warning("‚ö†Ô∏è **RISCO M√âDIO**: Monitoramento cont√≠nuo recomendado. Considerar a√ß√µes preventivas.")
        elif result["risk_score"] >= 30:
            st.info("‚ÑπÔ∏è **RISCO BAIXO**: Monitoramento regular suficiente.")
        else:
            st.success("‚úÖ **RISCO NULO**: Situa√ß√£o controlada. Manter procedimentos padr√£o.")

if __name__ == "__main__":
    main()
